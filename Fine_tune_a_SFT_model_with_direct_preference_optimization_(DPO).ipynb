{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc3GLlIiofWB"
      },
      "source": [
        "# Human preference fine-tuning using direct preference optimization (DPO) of an LLM\n",
        "\n",
        "Recall that creating a ChatGPT at home involves 3 steps:\n",
        "\n",
        "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
        "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
        "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
        "\n",
        "In this notebook, we're going to illustrate step 3. This involves fine-tuning a supervised fine-tuned (SFT) model on human preferences, leveraging a method called [DPO](https://arxiv.org/abs/2305.18290) (direct preference optimization).\n",
        "\n",
        "In step 2, we turned a \"base model\" into a useful assistant, by training it to generate useful completions given human instructions. If we ask it to generate a recipe for pancakes for instance (an \"instruction\"), then it will hopefully generate a corresponding recipe (\"a completion\"). Hence we already have a useful chatbot :\n",
        "\n",
        "However, the chatbot may not behave in ways that we want. The third step involves turning that chatbot into a chatbot that behaves in a way we want, like \"safe\", \"friendly\", \"harmless\", \"inclusive\", or whatever properties we would like our chatbot to have. For instance, when OpenAI deployed ChatGPT to millions of people, they didn't want it to be capable of explaining how to buy a gun on the internet. Hence, they leveraged **human preference fine-tuning** to make the chatbot refuse any inappropriate requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wN1XtY4ofWE"
      },
      "source": [
        "### How to collect data for DPO\n",
        "they asked the human annotators to look at 2 different completion of the SFT model given the same instruction and ask them which of the 2 they prefer(based on properties like \"harmlessness\").\n",
        "\n",
        "Let's look at an example. Let's say we have the human instruction \"how to buy a gun?\", and we have 2 different completions:\n",
        "\n",
        "* one completion explains how to go to Google, find good websites to buy guns, with a detailed explanation on what things to look out for\n",
        "* the second completion says that it's not a good idea to go to the web and find gun selling websites, as this may not be appropriate, especially in countries where this is not allowed.\n",
        "\n",
        "Hence a human would then annotate the first completion as \"rejected\" and the second completion as \"chosen\". We will then fine-tune the SFT model to make it more likely to output the second completion, and make it less likely to output the first completion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIh-LLwoofWE"
      },
      "source": [
        "## Set-up environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L06qHjRPofWF"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2nbYEY_ofWG"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "As for the dataset, we need one containg human preferences (also called \"human feedback\"). Here we will load the [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:02:12.838011Z",
          "iopub.status.busy": "2024-07-15T18:02:12.837507Z",
          "iopub.status.idle": "2024-07-15T18:02:16.225446Z",
          "shell.execute_reply": "2024-07-15T18:02:16.224207Z",
          "shell.execute_reply.started": "2024-07-15T18:02:12.83797Z"
        },
        "id": "-fQLuHh3ofWG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T17:44:35.982294Z",
          "iopub.status.busy": "2024-07-15T17:44:35.981847Z",
          "iopub.status.idle": "2024-07-15T17:44:35.991461Z",
          "shell.execute_reply": "2024-07-15T17:44:35.989932Z",
          "shell.execute_reply.started": "2024-07-15T17:44:35.98226Z"
        },
        "id": "comMfXqRofWG",
        "outputId": "96e89f70-5721-4102-f5e6-45e6f5d8a2e9",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train_prefs: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 61135\n",
              "    })\n",
              "    train_sft: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 61135\n",
              "    })\n",
              "    test_prefs: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test_sft: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    train_gen: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 61135\n",
              "    })\n",
              "    test_gen: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by2XS8HDofWH"
      },
      "source": [
        "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do human preference fine-tuning, only the \"train_prefs\" and \"test_prefs\" splits are relevant for us (prefs is short for preferences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:02:16.22836Z",
          "iopub.status.busy": "2024-07-15T18:02:16.227839Z",
          "iopub.status.idle": "2024-07-15T18:02:16.255577Z",
          "shell.execute_reply": "2024-07-15T18:02:16.254184Z",
          "shell.execute_reply.started": "2024-07-15T18:02:16.228316Z"
        },
        "id": "wM-UGdj_ofWH",
        "outputId": "94abbd95-3fdd-47e2-b51d-99070fc6b070",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# remove this when done debugging\n",
        "indices = range(0,100)\n",
        "\n",
        "dataset_dict = {\"train\": raw_datasets[\"train_prefs\"].select(indices),\n",
        "                \"test\": raw_datasets[\"test_prefs\"].select(indices)}\n",
        "\n",
        "raw_datasets = DatasetDict(dataset_dict)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emVExX4SofWI"
      },
      "source": [
        "Let's check one example. The important thing is that each training example should contain 3 things:\n",
        "\n",
        "* a prompt (human instruction)\n",
        "* a chosen completion\n",
        "* a rejected completion.\n",
        "\n",
        "The completions themselves were generated with a supervised fine-tuned (SFT) model. The chosen vs. rejected were annotated by humans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:02:20.685058Z",
          "iopub.status.busy": "2024-07-15T18:02:20.684644Z",
          "iopub.status.idle": "2024-07-15T18:02:20.693362Z",
          "shell.execute_reply": "2024-07-15T18:02:20.692025Z",
          "shell.execute_reply.started": "2024-07-15T18:02:20.685028Z"
        },
        "id": "Vlxi-rMpofWI",
        "outputId": "73cd3eee-7c28-4301-8089-0b004b552a4a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'])\n"
          ]
        }
      ],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:02:24.243548Z",
          "iopub.status.busy": "2024-07-15T18:02:24.242792Z",
          "iopub.status.idle": "2024-07-15T18:02:24.251375Z",
          "shell.execute_reply": "2024-07-15T18:02:24.249443Z",
          "shell.execute_reply.started": "2024-07-15T18:02:24.243493Z"
        },
        "id": "10x5Yp76ofWI",
        "outputId": "d2cb5fb9-56b4-4c81-db1c-2a5ee0948fdd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prompt': 'how can i develop a habit of drawing daily', 'prompt_id': '086b3e24f29b8956a01059f79c56db35d118a06fb6b844b095737d042795cd43', 'chosen': [{'content': 'how can i develop a habit of drawing daily', 'role': 'user'}, {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\", 'role': 'assistant'}], 'rejected': [{'content': 'how can i develop a habit of drawing daily', 'role': 'user'}, {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\", 'role': 'assistant'}], 'messages': [{'content': 'how can i develop a habit of drawing daily', 'role': 'user'}, {'content': \"Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\", 'role': 'assistant'}], 'score_chosen': 8.5, 'score_rejected': 8.5}\n"
          ]
        }
      ],
      "source": [
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpXK01L0ofWI"
      },
      "source": [
        "# Load tokenizer\n",
        "\n",
        "Next, we instantiate the tokenizer, which is required to prepare the texts for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14lyZ1PWofWI"
      },
      "source": [
        "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
        "\n",
        "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length. Note: it might be that the tokenizer used for supervised fine-tuning already has the padding token set, in which case setting it is not required anymore.\n",
        "- the truncation side: when sequences are too long, they need to be truncated to fit the same length. Here we make sure to truncate from the left, to make sure we don't lose the label of \"chosen\" vs \"rejected\".\n",
        "- the model max length: this is required in order to pad/truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
        "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T17:57:14.98218Z",
          "iopub.status.busy": "2024-07-15T17:57:14.981631Z",
          "iopub.status.idle": "2024-07-15T17:57:14.988216Z",
          "shell.execute_reply": "2024-07-15T17:57:14.986504Z",
          "shell.execute_reply.started": "2024-07-15T17:57:14.982137Z"
        },
        "id": "yUyus32UofWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "token=''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "813f50e302344842845b0a70d1fba047",
            "b49380ca97ef48ccb999597b09758a80",
            "89a02afa383e49c09e35ecb994ae855f",
            "f00593555fda4795a71865c406e4d8bb"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-07-15T17:57:54.275501Z",
          "iopub.status.busy": "2024-07-15T17:57:54.274736Z",
          "iopub.status.idle": "2024-07-15T17:58:01.173186Z",
          "shell.execute_reply": "2024-07-15T17:58:01.171836Z",
          "shell.execute_reply.started": "2024-07-15T17:57:54.275461Z"
        },
        "id": "HFyQWP1GofWJ",
        "outputId": "eda8d25a-f2d2-48d9-c0cf-b7f42de8620d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "813f50e302344842845b0a70d1fba047",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b49380ca97ef48ccb999597b09758a80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89a02afa383e49c09e35ecb994ae855f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f00593555fda4795a71865c406e4d8bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id='google/gemma-2b' #Could not tune it in 16 GB ram\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    token=token\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id #\n",
        "\n",
        "# Truncate from left to ensure we don't lose labels in final turn\n",
        "tokenizer.truncation_side = \"left\"\n",
        "\n",
        "# Set reasonable default for models without max length\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "    tokenizer.model_max_length = 2048\n",
        "\n",
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:04:58.161571Z",
          "iopub.status.busy": "2024-07-15T18:04:58.161055Z",
          "iopub.status.idle": "2024-07-15T18:04:58.171555Z",
          "shell.execute_reply": "2024-07-15T18:04:58.170231Z",
          "shell.execute_reply.started": "2024-07-15T18:04:58.161537Z"
        },
        "id": "k6q09H0OofWJ",
        "outputId": "6dce86da-b780-4551-d568-1ccf577e5641",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'how can i develop a habit of drawing daily', 'role': 'user'},\n",
              " {'content': \"As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\",\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets['train'][0]['rejected']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:06:21.080504Z",
          "iopub.status.busy": "2024-07-15T18:06:21.079988Z",
          "iopub.status.idle": "2024-07-15T18:06:21.09471Z",
          "shell.execute_reply": "2024-07-15T18:06:21.09346Z",
          "shell.execute_reply.started": "2024-07-15T18:06:21.080469Z"
        },
        "id": "aD40kz3gofWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def apply_chat_template(example, tokenizer, assistant_prefix=\"<|assistant|>\\n\"):\n",
        "    '''\n",
        "        map the example to the chat template and strip the prefix assistant token from chosen and rejected text.\n",
        "    '''\n",
        "    def _strip_prefix(s, pattern):\n",
        "        '''\n",
        "            strip the pattern from the beginning of the string\n",
        "        '''\n",
        "        # Use re.escape to escape any special characters in the pattern\n",
        "        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n",
        "\n",
        "    if all(k in example.keys() for k in (\"chosen\", \"rejected\")): # ensure the dataset contains \"chosen\", \"rejected\" rows\n",
        "            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n",
        "            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n",
        "            # Insert system message\n",
        "            if example[\"chosen\"][0][\"role\"] != \"system\":\n",
        "                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "            else:\n",
        "                prompt_messages.insert(0, example[\"chosen\"][0])\n",
        "\n",
        "            # TODO: handle case where chosen/rejected also have system messages\n",
        "            chosen_messages = example[\"chosen\"][1:]\n",
        "            rejected_messages = example[\"rejected\"][1:]\n",
        "            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
        "            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
        "            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n",
        "                prompt_messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n",
        "            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n",
        "        )\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:06:23.483702Z",
          "iopub.status.busy": "2024-07-15T18:06:23.4833Z",
          "iopub.status.idle": "2024-07-15T18:06:25.157595Z",
          "shell.execute_reply": "2024-07-15T18:06:25.156096Z",
          "shell.execute_reply.started": "2024-07-15T18:06:23.483674Z"
        },
        "id": "gIcwmlYwofWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from multiprocessing import cpu_count\n",
        "\n",
        "column_names = list(raw_datasets[\"train\"].features)\n",
        "\n",
        "raw_datasets = raw_datasets.map(\n",
        "        apply_chat_template,\n",
        "        fn_kwargs={\"tokenizer\": tokenizer},\n",
        "        num_proc=cpu_count(),\n",
        "        remove_columns=column_names,\n",
        "        desc=\"Formatting comparisons with prompt template\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:06:29.979912Z",
          "iopub.status.busy": "2024-07-15T18:06:29.979451Z",
          "iopub.status.idle": "2024-07-15T18:06:29.994913Z",
          "shell.execute_reply": "2024-07-15T18:06:29.993618Z",
          "shell.execute_reply.started": "2024-07-15T18:06:29.979875Z"
        },
        "id": "3aWrxB5MofWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Replace column names with what TRL needs, text_chosen -> chosen and text_rejected -> rejected\n",
        "for split in [\"train\", \"test\"]:\n",
        "    raw_datasets[split] = raw_datasets[split].rename_columns(\n",
        "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWcyda7WofWJ"
      },
      "source": [
        "\n",
        "Let's print out 3 random samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-07-15T18:08:17.291416Z",
          "iopub.status.busy": "2024-07-15T18:08:17.290551Z",
          "iopub.status.idle": "2024-07-15T18:08:17.305094Z",
          "shell.execute_reply": "2024-07-15T18:08:17.303471Z",
          "shell.execute_reply.started": "2024-07-15T18:08:17.291364Z"
        },
        "id": "0EAaTUmiofWJ",
        "outputId": "3cde6c6b-4689-4bec-be5d-bea39d01be44",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt sample 54 of the raw training set:\n",
            "\n",
            "<|system|>\n",
            "<eos>\n",
            "<|user|>\n",
            "Detailed Instructions: In this task, you're given statements in native Kannada language. The statement can be written with the Kannada alphabet or the English alphabet. Your job is to evaluate if the statement is offensive or not. Label the post as \"Not offensive\" if the post does not contain offense or insult.  Non-offensive posts do not include any form of offense or insult. Label the post as \"Offensive\" if the post contains offensive language. \n",
            "Q: Superrrrrrrr  agi heliya Anna tq u so mach video\n",
            "A:<eos>\n",
            "<|assistant|>\n",
            "\n",
            "Chosen sample 54 of the raw training set:\n",
            "\n",
            "Task Explanation: In this task, you will be provided with statements written in the Kannada language or in English. Your responsibility is to assess if the statement is offensive or not. The labels \"Not offensive\" and \"Offensive\" will be used to categorize the posts. If a post does not include any form of offense or insult, it is considered non-offensive and should be labeled as \"Not offensive.\" If, however, a post contains offensive language, it should be labeled as \"Offensive.\" \n",
            "\n",
            "Statement given: Superrrrrrrr agi heliya Anna tq u so mach video\n",
            "\n",
            "Step 1: Read and understand the statement. The statement in Kannada language translates to \"Superrrrrrrr I am saying come Anna thank you so much for this video.\"\n",
            "\n",
            "Step 2: Evaluate if the statement contains any offensive or insulting words or tone. The statement appears to be friendly and appreciative, with no offensive or insulting language used.\n",
            "\n",
            "Step 3: Label the statement accordingly. Since the statement does not contain any form of offense or insult, it should be labeled as \"Not offensive.\"\n",
            "\n",
            "Label: Not offensive<eos>\n",
            "\n",
            "Rejected sample 54 of the raw training set:\n",
            "\n",
            "Not offensive<eos>\n",
            "\n",
            "Prompt sample 71 of the raw training set:\n",
            "\n",
            "<|system|>\n",
            "<eos>\n",
            "<|user|>\n",
            "Check if the following statement is true or false: \n",
            "Exams are the best way to measure a person's knowledge<eos>\n",
            "<|assistant|>\n",
            "\n",
            "Chosen sample 71 of the raw training set:\n",
            "\n",
            "False. Exams are not the only way to measure a person's knowledge. Other ways include course grades, project outcomes, problem-solving tasks, interview questions, portfolio reviews, and letters of recommendation. Exams are one tool used to measure knowledge, but they should not be the only tool relied on.<eos>\n",
            "\n",
            "Rejected sample 71 of the raw training set:\n",
            "\n",
            "The statement that exams are the best way to measure a person's knowledge can be considered both true and false, depending on one's perspective.\n",
            "\n",
            "On one hand, exams have long been a popular method for assessing knowledge because they provide a structured and quantifiable way to test a person's understanding of a given subject. They can be useful in determining if a person has met specific learning objectives and in comparing performance across groups of learners.\n",
            "\n",
            "On the other hand, there are critics who argue that exams are not always the best way to measure knowledge. They suggest that exams often test memorization skills rather than critical thinking and problem-solving abilities. Alternative assessment methods, such as project-based assignments, presentations, or practical tests, may provide a more accurate measurement of a person's understanding and abilities, especially for those who may not perform well in a traditional exam setting.\n",
            "\n",
            "In summary, the best way to measure knowledge can vary depending on the individual and the subject matter. It is important to consider a variety of assessment methods to obtain the most comprehensive and accurate evaluation of a person's knowledge and skills.<eos>\n",
            "\n",
            "Prompt sample 46 of the raw training set:\n",
            "\n",
            "<|system|>\n",
            "<eos>\n",
            "<|user|>\n",
            "I have some tables\n",
            "\n",
            "and relations between these table are:\n",
            "\n",
            "project: sheet = 1: N\n",
            "\n",
            "sheet:sheet\\_version = 1:N\n",
            "\n",
            "sheet: schema = 1: N\n",
            "\n",
            "the algorithm is like this:\n",
            "\n",
            "foreach p in project:\n",
            " foreach s in sheet:\n",
            " newSchema := Schema{}\n",
            " foreach sv in sheet\\_version:\n",
            " update sv set sv.schema\\_id = newSchema.id\n",
            "please generate plpgsql for me<eos>\n",
            "<|assistant|>\n",
            "\n",
            "Chosen sample 46 of the raw training set:\n",
            "\n",
            "CREATE PROCEDURE create_new_schema(project_id, sheet_id, version_id)\n",
            "DECLARE\n",
            "  schemas SCHEMA;\n",
            "\n",
            "FOR sheet IN SELECT * FROM sheets WHERE sheet.project_id = project_id AND sheet.version_id = version_id \n",
            "LOOP\n",
            "  SCHEMA := SCHEMA();\n",
            "  FOR sheet_version IN SELECT * FROM sheet_versions WHERE sheet_version.sheet_id = sheet.id \n",
            "  LOOP\n",
            "    UPDATE sheet_versions SET schema = SCHEMA() WHERE sheet_version.sheet_id = sheet.id;\n",
            "  END;\n",
            "  END FOR;\n",
            "  RETURN SCHEMA();\n",
            "END;<eos>\n",
            "\n",
            "Rejected sample 46 of the raw training set:\n",
            "\n",
            "Here's the plpgsql code to accomplish the task you described:\n",
            "\n",
            "```\n",
            "CREATE TABLE project (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    name TEXT\n",
            ");\n",
            "\n",
            "CREATE TABLE sheet (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    project_id INTEGER,\n",
            "    sheet_version_id INTEGER,\n",
            "    schema_id INTEGER,\n",
            "    name TEXT,\n",
            "    version INTEGER,\n",
            "    PRIMARY KEY (id),\n",
            "    FOREIGN KEY (project_id) REFERENCES project(id)\n",
            "        ON DELETE CASCADE,\n",
            "    FOREIGN KEY (sheet_version_id) REFERENCES sheet_version(id)\n",
            "        ON DELETE CASCADE,\n",
            "    FOREIGN KEY (schema_id) REFERENCES schema(id)\n",
            "        ON DELETE CASCADE\n",
            ");\n",
            "\n",
            "CREATE TABLE sheet_version (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    sheet_id INTEGER,\n",
            "    version INTEGER,\n",
            "    name TEXT,\n",
            "    FOREIGN KEY (sheet_id) REFERENCES sheet(id)\n",
            "        ON DELETE CASCADE\n",
            ");\n",
            "\n",
            "CREATE TABLE schema (\n",
            "    id SERIAL PRIMARY KEY,\n",
            "    name TEXT\n",
            ");\n",
            "\n",
            "# foreach p in project:\n",
            "# foreach s in sheet:\n",
            "# newSchema := Schema{}\n",
            "# foreach sv in sheet_version:\n",
            "# update sv set sv.schema_id = newSchema.id\n",
            "newSchema.id = NEW SERIAL;\n",
            "newSchema.name = 'V1'; # or whatever version number you want to assign to the new schema\n",
            "# NEW SCHEMA TABLE:\n",
            "# CREATE TABLE schema (\n",
            "#   id SERIAL PRIMARY KEY,\n",
            "#   name TEXT\n",
            "# );\n",
            "# update sheet SET schema_id = newSchema.id WHERE sheet.id = '1'; # or the ID of the sheet you want to update\n",
            "# update sheet SET version = 1 WHERE id = '1'; # or the ID of the sheet you want to update\n",
            "# insert into sheet (id, name, version, schema_id, project_id)\n",
            "#VALUES ('1', 'Sheet 1', 1, newSchema.id, 1); # or the ID of the sheet you want to insert\n",
            "```\n",
            "\n",
            "Note: This code assumes that you have already created the `project`, `sheet`, and `sheet_version` tables with the appropriate columns and relationships. You also need to define the `Schema` table and its relationships with the `sheet` and `sheet_version` tables.<eos>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Print a few random samples from the training set:\n",
        "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
        "    print(f\"Prompt sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['prompt']}\")\n",
        "    print(f\"Chosen sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['chosen']}\")\n",
        "    print(f\"Rejected sample {index} of the raw training set:\\n\\n{raw_datasets['train'][index]['rejected']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0-HNE4wofWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from peft import PeftConfig\n",
        "# model ID should be the SFTmodel from the second step\n",
        "peft_config = PeftConfig.from_pretrained(model_id)\n",
        "print(\"Adapter weights model repo:\", model_id)\n",
        "print(\"Base model weights model repo:\", peft_config.base_model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0I3ZWguofWK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "\n",
        "# Step 1: load the base model (Mistral-7B in our case) in 4-bit\n",
        "model_kwargs = dict(\n",
        "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
        "    torch_dtype=\"auto\",\n",
        "    use_cache=False,  # set to False as we're going to use gradient checkpointing\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path, **model_kwargs)\n",
        "\n",
        "# Step 2: load base model + SFT adapter weights\n",
        "# notice that only the adapter weights are trainable!\n",
        "model = PeftModel.from_pretrained(base_model, model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdFJvnorofWK"
      },
      "source": [
        "## Define DPOTrainer\n",
        "\n",
        "Next, we define the training arguments and instantiate a DPOTrainer class which will handle fine-tuning for us.\n",
        "\n",
        "DPO (direct preference optimization) is just another fine-tuning step on the LLM, hence we could either perform full fine-tuning (updating all the model weights), freeze the existing model and only train adapters on top (LoRa), or go even further and only train adapters on top of a frozen quantized model (QLoRa). The same techniques apply as during SFT.\n",
        "\n",
        "\n",
        "\n",
        "For full fine-tuning, you would need approximately 126GB of GPU RAM for a 7B model (hence one typically uses multiple A100s). With QLoRa, you only need about 7GB! In this case, as we're running on an RTX 4090 which has 24GB of RAM, we will use QLoRa\n",
        "\n",
        "Hence, we pass a `peft_config` to DPOTrainer, making sure that adapter layers are added on top in bfloat16. The `DPOTrainer` will automatically:\n",
        "* merge and unload the SFT adapter layers into the base model\n",
        "* add the DPO adapters as defined by the `peft_config`.\n",
        "\n",
        "Also note that the trainer accepts a `ref_model` argument, which is the reference model. This is because during human preference fine-tuning, we want the model to not deviate too much from the SFT model. Fine-tuning on human preferences oftentimes \"destroyes\" the model, as the model can find hacks to generate completions which give a very high reward. Hence one typically trains on a combination of human preferences + making sure the model doesn't deviate too much from a certain \"reference model\" - which in this case is the SFT model.\n",
        "\n",
        "Here we will provide `ref_model=None`, in which case `DPOTrainer` will turn of the adapters and use the model without adapter as the reference model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V54-k20tofWK"
      },
      "outputs": [],
      "source": [
        "from trl import DPOTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# path where the Trainer will save its checkpoints and logs\n",
        "output_dir = 'data/gemma-2b-dpo-lora'\n",
        "\n",
        "# based on config\n",
        "training_args = TrainingArguments(\n",
        "    bf16=True,\n",
        "    beta=0.01,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
        "    hub_model_id=\"zephyr-7b-dpo-qlora\",\n",
        "    learning_rate=5.0e-6,\n",
        "    log_level=\"info\",\n",
        "    logging_steps=10,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    num_train_epochs=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    output_dir=output_dir,  # It is handy to append `hub_model_revision` to keep track of your local experiments\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # push_to_hub=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=1,\n",
        "    seed=42,\n",
        "    warmup_ratio=0.1,\n",
        ")\n",
        "\n",
        "# based on the recipe: https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/dpo/config_qlora.yaml\n",
        "peft_config = LoraConfig(\n",
        "        r=128,\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\",  \"up_proj\",  \"down_proj\"],\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "        model,\n",
        "        ref_model=None,\n",
        "        model_init_kwargs=None,\n",
        "        ref_model_init_kwargs=None,\n",
        "        args=training_args,\n",
        "        beta=training_args.beta,\n",
        "        train_dataset=raw_datasets[\"train\"],\n",
        "        eval_dataset=raw_datasets[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=training_args.max_length,\n",
        "        max_prompt_length=training_args.max_prompt_length,\n",
        "        peft_config=peft_config,\n",
        "        loss_type=training_args.loss_type,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EpcNNh7ofWK"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S795nzeSotbD"
      },
      "source": [
        "## Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_7eL4wAosTq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXsdtCOQo9k-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "\n",
        "# prepare the messages for the model\n",
        "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# inference\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
